{"id":"b8e3c334-45bb-48d9-9b06-8ea4bd6ae788","data":{"nodes":[{"id":"ChatInput-lNGfy","type":"genericNode","position":{"x":-319.76587249477996,"y":-422.7113885229494},"data":{"type":"ChatInput","node":{"template":{"_type":"Component","files":{"trace_as_metadata":true,"file_path":"","fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"files","display_name":"Files","advanced":true,"dynamic":false,"info":"Files to be sent with the message.","title_case":false,"type":"file","_input_type":"FileInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"How many users are in the database?","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"},"sender":{"combobox":false,"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"User","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"User","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"should_store_message","display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Get chat inputs from the Playground.","icon":"ChatInput","base_classes":["Message"],"display_name":"Chat Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","files"],"beta":false,"edited":false},"id":"ChatInput-lNGfy","description":"Get chat inputs from the Playground.","display_name":"Chat Input"},"selected":false,"width":384,"height":317,"positionAbsolute":{"x":-319.76587249477996,"y":-422.7113885229494},"dragging":false},{"id":"ChatOutput-VM2j6","type":"genericNode","position":{"x":3198.5946047188145,"y":-199.65876256182088},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_AI\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"{text}","name":"data_template","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"sender":{"combobox":false,"trace_as_metadata":true,"options":["Machine","User"],"required":false,"placeholder":"","show":true,"value":"Machine","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"AI","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"should_store_message","display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template"],"beta":false,"edited":false},"id":"ChatOutput-VM2j6","description":"Display a chat message in the Playground.","display_name":"Chat Output"},"selected":false,"width":384,"height":317,"positionAbsolute":{"x":3198.5946047188145,"y":-199.65876256182088},"dragging":false},{"id":"Prompt-WABLE","type":"genericNode","position":{"x":238.89961658180982,"y":7.350949815898787},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":"Given the following database schema:\n{database_schema}\n\nGenerate a SQL query to answer the following question:\n{question}\n\nThe query should be valid SQL and should only use the tables and columns provided in the schema. Output only the SQL query, without any explanation or additional text. DO NOT USE backticks (```) \n\nSQL Query:","name":"template","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"database_schema":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"database_schema","display_name":"database_schema","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"question":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"question","display_name":"question","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","base_classes":["Message"],"display_name":"Prompt","documentation":"","custom_fields":{"template":["database_schema","question"]},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"edited":false},"id":"Prompt-WABLE","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":false,"width":384,"height":525,"positionAbsolute":{"x":238.89961658180982,"y":7.350949815898787},"dragging":false},{"id":"OllamaModel-VzsWa","type":"genericNode","position":{"x":756.5500453155792,"y":72.46454151770465},"data":{"type":"OllamaModel","node":{"template":{"_type":"Component","base_url":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"http://ollama:11434","name":"base_url","display_name":"Base URL","advanced":false,"dynamic":false,"info":"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.","title_case":false,"type":"str","_input_type":"StrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import Any\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value + \"/api/tags\")\n\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, url: str) -> list[str]:\n        try:\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n                return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    inputs = LCModelComponent._base_inputs + [\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3\",\n            info=\"Refer to https://ollama.ai/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\",\n            display_name=\"Format\",\n            info=\"Specify the format of the output (e.g., json).\",\n            advanced=True,\n        ),\n        DictInput(\n            name=\"metadata\",\n            display_name=\"Metadata\",\n            info=\"Metadata to add to the run trace.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"tfs_z\",\n            display_name=\"TFS Z\",\n            info=\"Tail free sampling value. (Default: 1)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"Timeout for the request stream.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Limits token selection to top K. (Default: 40)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"Works together with top-k. (Default: 0.9)\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            info=\"Whether to print out response text.\",\n        ),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"system\",\n            display_name=\"System\",\n            info=\"System to use for generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"Template to use for generating text.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)  # type: ignore\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not initialize Ollama LLM.\") from e\n\n        return output  # type: ignore\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"format":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"format","display_name":"Format","advanced":true,"dynamic":false,"info":"Specify the format of the output (e.g., json).","title_case":false,"type":"str","_input_type":"StrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"metadata":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"metadata","display_name":"Metadata","advanced":true,"dynamic":false,"info":"Metadata to add to the run trace.","title_case":false,"type":"dict","_input_type":"DictInput"},"mirostat":{"combobox":false,"trace_as_metadata":true,"options":["Disabled","Mirostat","Mirostat 2.0"],"required":false,"placeholder":"","show":true,"value":"Disabled","name":"mirostat","display_name":"Mirostat","advanced":true,"dynamic":false,"info":"Enable/disable Mirostat sampling for controlling perplexity.","real_time_refresh":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"mirostat_eta":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"mirostat_eta","display_name":"Mirostat Eta","advanced":true,"dynamic":false,"info":"Learning rate for Mirostat algorithm. (Default: 0.1)","title_case":false,"type":"float","_input_type":"FloatInput"},"mirostat_tau":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"mirostat_tau","display_name":"Mirostat Tau","advanced":true,"dynamic":false,"info":"Controls the balance between coherence and diversity of the output. (Default: 5.0)","title_case":false,"type":"float","_input_type":"FloatInput"},"model_name":{"combobox":false,"trace_as_metadata":true,"options":["mxbai-embed-large:latest","llama3:latest","gemma2:latest"],"required":false,"placeholder":"","show":true,"value":"llama3:latest","name":"model_name","display_name":"Model Name","advanced":false,"dynamic":false,"info":"Refer to https://ollama.ai/library for more models.","refresh_button":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"num_ctx":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"num_ctx","display_name":"Context Window Size","advanced":true,"dynamic":false,"info":"Size of the context window for generating tokens. (Default: 2048)","title_case":false,"type":"int","_input_type":"IntInput"},"num_gpu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"num_gpu","display_name":"Number of GPUs","advanced":true,"dynamic":false,"info":"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)","title_case":false,"type":"int","_input_type":"IntInput"},"num_thread":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"num_thread","display_name":"Number of Threads","advanced":true,"dynamic":false,"info":"Number of threads to use during computation. (Default: detected for optimal performance)","title_case":false,"type":"int","_input_type":"IntInput"},"repeat_last_n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"repeat_last_n","display_name":"Repeat Last N","advanced":true,"dynamic":false,"info":"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)","title_case":false,"type":"int","_input_type":"IntInput"},"repeat_penalty":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"repeat_penalty","display_name":"Repeat Penalty","advanced":true,"dynamic":false,"info":"Penalty for repetitions in generated text. (Default: 1.1)","title_case":false,"type":"float","_input_type":"FloatInput"},"stop_tokens":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"stop_tokens","display_name":"Stop Tokens","advanced":true,"dynamic":false,"info":"Comma-separated list of tokens to signal the model to stop generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system","display_name":"System","advanced":true,"dynamic":false,"info":"System to use for generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"tags":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"tags","display_name":"Tags","advanced":true,"dynamic":false,"info":"Comma-separated list of tags to add to the run trace.","title_case":false,"type":"str","_input_type":"StrInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.2,"name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"Controls the creativity of model responses.","title_case":false,"type":"float","_input_type":"FloatInput"},"template":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"template","display_name":"Template","advanced":true,"dynamic":false,"info":"Template to use for generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"tfs_z":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"tfs_z","display_name":"TFS Z","advanced":true,"dynamic":false,"info":"Tail free sampling value. (Default: 1)","title_case":false,"type":"float","_input_type":"FloatInput"},"timeout":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"timeout","display_name":"Timeout","advanced":true,"dynamic":false,"info":"Timeout for the request stream.","title_case":false,"type":"int","_input_type":"IntInput"},"top_k":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"top_k","display_name":"Top K","advanced":true,"dynamic":false,"info":"Limits token selection to top K. (Default: 40)","title_case":false,"type":"int","_input_type":"IntInput"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"top_p","display_name":"Top P","advanced":true,"dynamic":false,"info":"Works together with top-k. (Default: 0.9)","title_case":false,"type":"float","_input_type":"FloatInput"},"verbose":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"verbose","display_name":"Verbose","advanced":false,"dynamic":false,"info":"Whether to print out response text.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Generate text using Ollama Local LLMs.","icon":"Ollama","base_classes":["LanguageModel","Message"],"display_name":"Ollama","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","base_url","model_name","temperature","format","metadata","mirostat","mirostat_eta","mirostat_tau","num_ctx","num_gpu","num_thread","repeat_last_n","repeat_penalty","tfs_z","timeout","top_k","top_p","verbose","tags","stop_tokens","system","template"],"beta":false,"edited":false},"id":"OllamaModel-VzsWa","description":"Generate text using Ollama Local LLMs.","display_name":"Ollama"},"selected":false,"width":384,"height":711,"positionAbsolute":{"x":756.5500453155792,"y":72.46454151770465},"dragging":false},{"id":"SQLExecutor-WaHYk","type":"genericNode","position":{"x":-334.9210886070483,"y":136.48729375420402},"data":{"type":"SQLExecutor","node":{"template":{"_type":"CustomComponent","add_error":{"type":"bool","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"value":false,"fileTypes":[],"file_path":"","password":false,"name":"add_error","display_name":"Add Error","advanced":false,"dynamic":false,"info":"Add the error to the result.","load_from_db":false,"title_case":false},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\nfrom langchain_community.utilities import SQLDatabase\n\nfrom langflow.custom import CustomComponent\nfrom langflow.field_typing import Text\n\n\nclass SQLExecutorComponent(CustomComponent):\n    display_name = \"SQL Executor\"\n    description = \"Execute SQL query.\"\n    name = \"SQLExecutor\"\n    beta: bool = True\n\n    def build_config(self):\n        return {\n            \"database_url\": {\n                \"display_name\": \"Database URL\",\n                \"info\": \"The URL of the database.\",\n            },\n            \"include_columns\": {\n                \"display_name\": \"Include Columns\",\n                \"info\": \"Include columns in the result.\",\n            },\n            \"passthrough\": {\n                \"display_name\": \"Passthrough\",\n                \"info\": \"If an error occurs, return the query instead of raising an exception.\",\n            },\n            \"add_error\": {\n                \"display_name\": \"Add Error\",\n                \"info\": \"Add the error to the result.\",\n            },\n        }\n\n    def clean_up_uri(self, uri: str) -> str:\n        if uri.startswith(\"postgresql://\"):\n            uri = uri.replace(\"postgresql://\", \"postgres://\")\n        return uri.strip()\n\n    def build(\n        self,\n        query: str,\n        database_url: str,\n        include_columns: bool = False,\n        passthrough: bool = False,\n        add_error: bool = False,\n    ) -> Text:\n        error = None\n        try:\n            database = SQLDatabase.from_uri(database_url)\n        except Exception as e:\n            raise ValueError(f\"An error occurred while connecting to the database: {e}\")\n        try:\n            tool = QuerySQLDataBaseTool(db=database)\n            result = tool.run(query, include_columns=include_columns)\n            self.status = result\n        except Exception as e:\n            result = str(e)\n            self.status = result\n            if not passthrough:\n                raise e\n            error = repr(e)\n\n        if add_error and error is not None:\n            result = f\"{result}\\n\\nError: {error}\\n\\nQuery: {query}\"\n        elif error is not None:\n            # Then we won't add the error to the result\n            # but since we are in passthrough mode, we will return the query\n            result = query\n\n        return result\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"database_url":{"type":"str","required":true,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"database_url","display_name":"Database URL","advanced":false,"dynamic":false,"info":"The URL of the database.","load_from_db":false,"title_case":false,"input_types":["Text"],"value":"postgresql://genaiuser:genaipassword@postgres:5432/genaidb"},"include_columns":{"type":"bool","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"value":false,"fileTypes":[],"file_path":"","password":false,"name":"include_columns","display_name":"Include Columns","advanced":false,"dynamic":false,"info":"Include columns in the result.","load_from_db":false,"title_case":false},"passthrough":{"type":"bool","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"value":false,"fileTypes":[],"file_path":"","password":false,"name":"passthrough","display_name":"Passthrough","advanced":false,"dynamic":false,"info":"If an error occurs, return the query instead of raising an exception.","load_from_db":false,"title_case":false},"query":{"type":"str","required":true,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"query","advanced":false,"dynamic":false,"info":"","load_from_db":false,"title_case":false,"input_types":["Text"],"value":"SELECT      t.table_name,     c.column_name,     c.data_type,     c.character_maximum_length,     c.is_nullable FROM      information_schema.tables t JOIN      information_schema.columns c      ON t.table_name = c.table_name     AND t.table_schema = c.table_schema WHERE      t.table_schema = current_schema()     AND t.table_type = 'BASE TABLE' ORDER BY      t.table_name,      c.ordinal_position;"}},"description":"Execute SQL query.","base_classes":["object","str","Text"],"display_name":"SQL Executor","documentation":"","custom_fields":{"query":null,"database_url":null,"include_columns":null,"passthrough":null,"add_error":null},"output_types":["Text"],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Text"],"selected":"Text","name":"text","hidden":null,"display_name":"Text","method":null,"value":"__UNDEFINED__","cache":true}],"field_order":[],"beta":true,"edited":false},"id":"SQLExecutor-WaHYk","description":"Execute SQL query.","display_name":"SQL Executor"},"selected":false,"width":384,"height":627,"positionAbsolute":{"x":-334.9210886070483,"y":136.48729375420402},"dragging":false},{"id":"SQLExecutor-yjMYd","type":"genericNode","position":{"x":1809.944370327626,"y":-35.80880902081367},"data":{"type":"SQLExecutor","node":{"template":{"_type":"CustomComponent","add_error":{"type":"bool","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"value":false,"fileTypes":[],"file_path":"","password":false,"name":"add_error","display_name":"Add Error","advanced":false,"dynamic":false,"info":"Add the error to the result.","load_from_db":false,"title_case":false},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\nfrom langchain_community.utilities import SQLDatabase\n\nfrom langflow.custom import CustomComponent\nfrom langflow.field_typing import Text\n\n\nclass SQLExecutorComponent(CustomComponent):\n    display_name = \"SQL Executor\"\n    description = \"Execute SQL query.\"\n    name = \"SQLExecutor\"\n    beta: bool = True\n\n    def build_config(self):\n        return {\n            \"database_url\": {\n                \"display_name\": \"Database URL\",\n                \"info\": \"The URL of the database.\",\n            },\n            \"include_columns\": {\n                \"display_name\": \"Include Columns\",\n                \"info\": \"Include columns in the result.\",\n            },\n            \"passthrough\": {\n                \"display_name\": \"Passthrough\",\n                \"info\": \"If an error occurs, return the query instead of raising an exception.\",\n            },\n            \"add_error\": {\n                \"display_name\": \"Add Error\",\n                \"info\": \"Add the error to the result.\",\n            },\n        }\n\n    def clean_up_uri(self, uri: str) -> str:\n        if uri.startswith(\"postgresql://\"):\n            uri = uri.replace(\"postgresql://\", \"postgres://\")\n        return uri.strip()\n\n    def build(\n        self,\n        query: str,\n        database_url: str,\n        include_columns: bool = False,\n        passthrough: bool = False,\n        add_error: bool = False,\n    ) -> Text:\n        error = None\n        try:\n            database = SQLDatabase.from_uri(database_url)\n        except Exception as e:\n            raise ValueError(f\"An error occurred while connecting to the database: {e}\")\n        try:\n            tool = QuerySQLDataBaseTool(db=database)\n            result = tool.run(query, include_columns=include_columns)\n            self.status = result\n        except Exception as e:\n            result = str(e)\n            self.status = result\n            if not passthrough:\n                raise e\n            error = repr(e)\n\n        if add_error and error is not None:\n            result = f\"{result}\\n\\nError: {error}\\n\\nQuery: {query}\"\n        elif error is not None:\n            # Then we won't add the error to the result\n            # but since we are in passthrough mode, we will return the query\n            result = query\n\n        return result\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"database_url":{"type":"str","required":true,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"database_url","display_name":"Database URL","advanced":false,"dynamic":false,"info":"The URL of the database.","load_from_db":false,"title_case":false,"input_types":["Text"],"value":"postgresql://genaiuser:genaipassword@postgres:5432/genaidb"},"include_columns":{"type":"bool","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"value":false,"fileTypes":[],"file_path":"","password":false,"name":"include_columns","display_name":"Include Columns","advanced":false,"dynamic":false,"info":"Include columns in the result.","load_from_db":false,"title_case":false},"passthrough":{"type":"bool","required":false,"placeholder":"","list":false,"show":true,"multiline":false,"value":false,"fileTypes":[],"file_path":"","password":false,"name":"passthrough","display_name":"Passthrough","advanced":false,"dynamic":false,"info":"If an error occurs, return the query instead of raising an exception.","load_from_db":false,"title_case":false},"query":{"type":"str","required":true,"placeholder":"","list":false,"show":true,"multiline":false,"fileTypes":[],"file_path":"","password":false,"name":"query","advanced":false,"dynamic":false,"info":"","load_from_db":false,"title_case":false,"input_types":["Text"]}},"description":"Execute SQL query.","base_classes":["object","str","Text"],"display_name":"SQL Executor","documentation":"","custom_fields":{"query":null,"database_url":null,"include_columns":null,"passthrough":null,"add_error":null},"output_types":["Text"],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Text"],"selected":"Text","name":"text","hidden":null,"display_name":"Text","method":null,"value":"__UNDEFINED__","cache":true}],"field_order":[],"beta":true,"edited":false},"id":"SQLExecutor-yjMYd"},"selected":false,"width":384,"height":627,"positionAbsolute":{"x":1809.944370327626,"y":-35.80880902081367},"dragging":false},{"id":"TextOutput-5zuMZ","type":"genericNode","position":{"x":1255.1683242942977,"y":114.39878844741375},"data":{"type":"TextOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MessageTextInput, Output\n\nclass TextOutputComponent(TextComponent):\n    display_name = \"Text Output\"\n    description = \"Display a text output in the Playground.\"\n    icon = \"type\"\n    name = \"TextOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as output.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\", type=\"str\"),\n    ]\n\n    def text_response(self) -> str:\n        self.status = self.input_value\n        return self.input_value","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as output.","title_case":false,"type":"str"}},"description":"Display a text output in the Playground.","icon":"type","base_classes":["Text"],"display_name":"Text Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Text"],"selected":"Text","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":true},"id":"TextOutput-5zuMZ","description":"Display a text output in the Playground.","display_name":"Text Output"},"selected":false,"width":384,"height":317,"positionAbsolute":{"x":1255.1683242942977,"y":114.39878844741375},"dragging":false},{"id":"Prompt-c6Wwu","type":"genericNode","position":{"x":2258.686654949791,"y":-207.12215474238533},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":"You are an AI assistant tasked with analyzing and summarizing SQL query results. Present your analysis using clear, well-formatted markdown. Your summary should include the original question, the raw data, key findings, and your interpretation.\n\nOriginal Question: {question}\n\nSQL Query Results:\n{response}\n\nInstructions:\n1. Use markdown formatting throughout your response, especially for structuring and presenting data.\n2. Include tables where appropriate, using proper markdown table syntax.\n3. Use headers, lists, and other markdown elements to organize information effectively.\n4. Present the original question, raw data, key findings, and your interpretation in a logical order.\n5. Ensure your markdown is clean and correctly formatted.\n6. Adapt your presentation style to best suit the data and findings.\n\nProvide your analysis below, using markdown to create a clear, well-structured, and easily readable summary.","name":"template","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"question":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"question","display_name":"question","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"response":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"response","display_name":"response","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","base_classes":["Message"],"display_name":"Prompt","documentation":"","custom_fields":{"template":["question","response"]},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"edited":false},"id":"Prompt-c6Wwu","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":false,"width":384,"height":525,"positionAbsolute":{"x":2258.686654949791,"y":-207.12215474238533},"dragging":false},{"id":"OllamaModel-RuDI1","type":"genericNode","position":{"x":2733.32906127671,"y":-278.651916833128},"data":{"type":"OllamaModel","node":{"template":{"_type":"Component","base_url":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"http://ollama:11434","name":"base_url","display_name":"Base URL","advanced":false,"dynamic":false,"info":"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.","title_case":false,"type":"str","_input_type":"StrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from typing import Any\n\nimport httpx\nfrom langchain_community.chat_models import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value + \"/api/tags\")\n\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, url: str) -> list[str]:\n        try:\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                model_names = [model[\"name\"] for model in data.get(\"models\", [])]\n                return model_names\n        except Exception as e:\n            raise ValueError(\"Could not retrieve models. Please, make sure Ollama is running.\") from e\n\n    inputs = LCModelComponent._base_inputs + [\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3\",\n            info=\"Refer to https://ollama.ai/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\",\n            display_name=\"Format\",\n            info=\"Specify the format of the output (e.g., json).\",\n            advanced=True,\n        ),\n        DictInput(\n            name=\"metadata\",\n            display_name=\"Metadata\",\n            info=\"Metadata to add to the run trace.\",\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"tfs_z\",\n            display_name=\"TFS Z\",\n            info=\"Tail free sampling value. (Default: 1)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"Timeout for the request stream.\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"top_k\",\n            display_name=\"Top K\",\n            info=\"Limits token selection to top K. (Default: 40)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"top_p\",\n            display_name=\"Top P\",\n            info=\"Works together with top-k. (Default: 0.9)\",\n            advanced=True,\n        ),\n        BoolInput(\n            name=\"verbose\",\n            display_name=\"Verbose\",\n            info=\"Whether to print out response text.\",\n        ),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"system\",\n            display_name=\"System\",\n            info=\"System to use for generating text.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"Template to use for generating text.\",\n            advanced=True,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)  # type: ignore\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)  # type: ignore\n        except Exception as e:\n            raise ValueError(\"Could not initialize Ollama LLM.\") from e\n\n        return output  # type: ignore\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"format":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"format","display_name":"Format","advanced":true,"dynamic":false,"info":"Specify the format of the output (e.g., json).","title_case":false,"type":"str","_input_type":"StrInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"metadata":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"metadata","display_name":"Metadata","advanced":true,"dynamic":false,"info":"Metadata to add to the run trace.","title_case":false,"type":"dict","_input_type":"DictInput"},"mirostat":{"combobox":false,"trace_as_metadata":true,"options":["Disabled","Mirostat","Mirostat 2.0"],"required":false,"placeholder":"","show":true,"value":"Disabled","name":"mirostat","display_name":"Mirostat","advanced":true,"dynamic":false,"info":"Enable/disable Mirostat sampling for controlling perplexity.","real_time_refresh":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"mirostat_eta":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"mirostat_eta","display_name":"Mirostat Eta","advanced":true,"dynamic":false,"info":"Learning rate for Mirostat algorithm. (Default: 0.1)","title_case":false,"type":"float","_input_type":"FloatInput"},"mirostat_tau":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"mirostat_tau","display_name":"Mirostat Tau","advanced":true,"dynamic":false,"info":"Controls the balance between coherence and diversity of the output. (Default: 5.0)","title_case":false,"type":"float","_input_type":"FloatInput"},"model_name":{"combobox":false,"trace_as_metadata":true,"options":["mxbai-embed-large:latest","llama3:latest","gemma2:latest"],"required":false,"placeholder":"","show":true,"value":"llama3:latest","name":"model_name","display_name":"Model Name","advanced":false,"dynamic":false,"info":"Refer to https://ollama.ai/library for more models.","refresh_button":true,"title_case":false,"type":"str","_input_type":"DropdownInput"},"num_ctx":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"num_ctx","display_name":"Context Window Size","advanced":true,"dynamic":false,"info":"Size of the context window for generating tokens. (Default: 2048)","title_case":false,"type":"int","_input_type":"IntInput"},"num_gpu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"num_gpu","display_name":"Number of GPUs","advanced":true,"dynamic":false,"info":"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)","title_case":false,"type":"int","_input_type":"IntInput"},"num_thread":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"num_thread","display_name":"Number of Threads","advanced":true,"dynamic":false,"info":"Number of threads to use during computation. (Default: detected for optimal performance)","title_case":false,"type":"int","_input_type":"IntInput"},"repeat_last_n":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"repeat_last_n","display_name":"Repeat Last N","advanced":true,"dynamic":false,"info":"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)","title_case":false,"type":"int","_input_type":"IntInput"},"repeat_penalty":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"repeat_penalty","display_name":"Repeat Penalty","advanced":true,"dynamic":false,"info":"Penalty for repetitions in generated text. (Default: 1.1)","title_case":false,"type":"float","_input_type":"FloatInput"},"stop_tokens":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"stop_tokens","display_name":"Stop Tokens","advanced":true,"dynamic":false,"info":"Comma-separated list of tokens to signal the model to stop generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system","display_name":"System","advanced":true,"dynamic":false,"info":"System to use for generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"tags":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"tags","display_name":"Tags","advanced":true,"dynamic":false,"info":"Comma-separated list of tags to add to the run trace.","title_case":false,"type":"str","_input_type":"StrInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.2,"name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"Controls the creativity of model responses.","title_case":false,"type":"float","_input_type":"FloatInput"},"template":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"template","display_name":"Template","advanced":true,"dynamic":false,"info":"Template to use for generating text.","title_case":false,"type":"str","_input_type":"StrInput"},"tfs_z":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"tfs_z","display_name":"TFS Z","advanced":true,"dynamic":false,"info":"Tail free sampling value. (Default: 1)","title_case":false,"type":"float","_input_type":"FloatInput"},"timeout":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"timeout","display_name":"Timeout","advanced":true,"dynamic":false,"info":"Timeout for the request stream.","title_case":false,"type":"int","_input_type":"IntInput"},"top_k":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"top_k","display_name":"Top K","advanced":true,"dynamic":false,"info":"Limits token selection to top K. (Default: 40)","title_case":false,"type":"int","_input_type":"IntInput"},"top_p":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"top_p","display_name":"Top P","advanced":true,"dynamic":false,"info":"Works together with top-k. (Default: 0.9)","title_case":false,"type":"float","_input_type":"FloatInput"},"verbose":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"verbose","display_name":"Verbose","advanced":false,"dynamic":false,"info":"Whether to print out response text.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Generate text using Ollama Local LLMs.","icon":"Ollama","base_classes":["LanguageModel","Message"],"display_name":"Ollama","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","base_url","model_name","temperature","format","metadata","mirostat","mirostat_eta","mirostat_tau","num_ctx","num_gpu","num_thread","repeat_last_n","repeat_penalty","tfs_z","timeout","top_k","top_p","verbose","tags","stop_tokens","system","template"],"beta":false,"edited":false},"id":"OllamaModel-RuDI1","description":"Generate text using Ollama Local LLMs.","display_name":"Ollama"},"selected":false,"width":384,"height":711,"positionAbsolute":{"x":2733.32906127671,"y":-278.651916833128},"dragging":false}],"edges":[{"source":"SQLExecutor-WaHYk","sourceHandle":"{dataType:SQLExecutor,id:SQLExecutor-WaHYk,name:text,output_types:[Text]}","target":"Prompt-WABLE","targetHandle":"{fieldName:database_schema,id:Prompt-WABLE,inputTypes:[Message,Text],type:str}","data":{"targetHandle":{"fieldName":"database_schema","id":"Prompt-WABLE","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"SQLExecutor","id":"SQLExecutor-WaHYk","name":"text","output_types":["Text"]}},"id":"reactflow__edge-SQLExecutor-WaHYk{dataType:SQLExecutor,id:SQLExecutor-WaHYk,name:text,output_types:[Text]}-Prompt-WABLE{fieldName:database_schema,id:Prompt-WABLE,inputTypes:[Message,Text],type:str}","className":""},{"source":"ChatInput-lNGfy","sourceHandle":"{dataType:ChatInput,id:ChatInput-lNGfy,name:message,output_types:[Message]}","target":"Prompt-WABLE","targetHandle":"{fieldName:question,id:Prompt-WABLE,inputTypes:[Message,Text],type:str}","data":{"targetHandle":{"fieldName":"question","id":"Prompt-WABLE","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-lNGfy","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-lNGfy{dataType:ChatInput,id:ChatInput-lNGfy,name:message,output_types:[Message]}-Prompt-WABLE{fieldName:question,id:Prompt-WABLE,inputTypes:[Message,Text],type:str}","className":""},{"source":"TextOutput-5zuMZ","sourceHandle":"{dataType:TextOutput,id:TextOutput-5zuMZ,name:text,output_types:[Text]}","target":"SQLExecutor-yjMYd","targetHandle":"{fieldName:query,id:SQLExecutor-yjMYd,inputTypes:[Text],type:str}","data":{"targetHandle":{"fieldName":"query","id":"SQLExecutor-yjMYd","inputTypes":["Text"],"type":"str"},"sourceHandle":{"dataType":"TextOutput","id":"TextOutput-5zuMZ","name":"text","output_types":["Text"]}},"id":"reactflow__edge-TextOutput-5zuMZ{dataType:TextOutput,id:TextOutput-5zuMZ,name:text,output_types:[Text]}-SQLExecutor-yjMYd{fieldName:query,id:SQLExecutor-yjMYd,inputTypes:[Text],type:str}","className":""},{"source":"SQLExecutor-yjMYd","sourceHandle":"{dataType:SQLExecutor,id:SQLExecutor-yjMYd,name:text,output_types:[Text]}","target":"Prompt-c6Wwu","targetHandle":"{fieldName:response,id:Prompt-c6Wwu,inputTypes:[Message,Text],type:str}","data":{"targetHandle":{"fieldName":"response","id":"Prompt-c6Wwu","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"SQLExecutor","id":"SQLExecutor-yjMYd","name":"text","output_types":["Text"]}},"id":"reactflow__edge-SQLExecutor-yjMYd{dataType:SQLExecutor,id:SQLExecutor-yjMYd,name:text,output_types:[Text]}-Prompt-c6Wwu{fieldName:response,id:Prompt-c6Wwu,inputTypes:[Message,Text],type:str}","className":""},{"source":"ChatInput-lNGfy","sourceHandle":"{dataType:ChatInput,id:ChatInput-lNGfy,name:message,output_types:[Message]}","target":"Prompt-c6Wwu","targetHandle":"{fieldName:question,id:Prompt-c6Wwu,inputTypes:[Message,Text],type:str}","data":{"targetHandle":{"fieldName":"question","id":"Prompt-c6Wwu","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-lNGfy","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-lNGfy{dataType:ChatInput,id:ChatInput-lNGfy,name:message,output_types:[Message]}-Prompt-c6Wwu{fieldName:question,id:Prompt-c6Wwu,inputTypes:[Message,Text],type:str}","className":""},{"source":"Prompt-WABLE","sourceHandle":"{dataType:Prompt,id:Prompt-WABLE,name:prompt,output_types:[Message]}","target":"OllamaModel-VzsWa","targetHandle":"{fieldName:input_value,id:OllamaModel-VzsWa,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"input_value","id":"OllamaModel-VzsWa","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-WABLE","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-WABLE{dataType:Prompt,id:Prompt-WABLE,name:prompt,output_types:[Message]}-OllamaModel-VzsWa{fieldName:input_value,id:OllamaModel-VzsWa,inputTypes:[Message],type:str}","className":""},{"source":"OllamaModel-VzsWa","sourceHandle":"{dataType:OllamaModel,id:OllamaModel-VzsWa,name:text_output,output_types:[Message]}","target":"TextOutput-5zuMZ","targetHandle":"{fieldName:input_value,id:TextOutput-5zuMZ,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"input_value","id":"TextOutput-5zuMZ","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OllamaModel","id":"OllamaModel-VzsWa","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OllamaModel-VzsWa{dataType:OllamaModel,id:OllamaModel-VzsWa,name:text_output,output_types:[Message]}-TextOutput-5zuMZ{fieldName:input_value,id:TextOutput-5zuMZ,inputTypes:[Message],type:str}","className":""},{"source":"Prompt-c6Wwu","sourceHandle":"{dataType:Prompt,id:Prompt-c6Wwu,name:prompt,output_types:[Message]}","target":"OllamaModel-RuDI1","targetHandle":"{fieldName:input_value,id:OllamaModel-RuDI1,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"input_value","id":"OllamaModel-RuDI1","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-c6Wwu","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-c6Wwu{dataType:Prompt,id:Prompt-c6Wwu,name:prompt,output_types:[Message]}-OllamaModel-RuDI1{fieldName:input_value,id:OllamaModel-RuDI1,inputTypes:[Message],type:str}","className":""},{"source":"OllamaModel-RuDI1","sourceHandle":"{dataType:OllamaModel,id:OllamaModel-RuDI1,name:text_output,output_types:[Message]}","target":"ChatOutput-VM2j6","targetHandle":"{fieldName:input_value,id:ChatOutput-VM2j6,inputTypes:[Message],type:str}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-VM2j6","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OllamaModel","id":"OllamaModel-RuDI1","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OllamaModel-RuDI1{dataType:OllamaModel,id:OllamaModel-RuDI1,name:text_output,output_types:[Message]}-ChatOutput-VM2j6{fieldName:input_value,id:ChatOutput-VM2j6,inputTypes:[Message],type:str}","className":""}],"viewport":{"x":179.26727717936524,"y":333.8379665829225,"zoom":0.38315499005194503}},"description":"Create, Chain, Communicate.","name":"Example 1 - Natural Language Query to SQL ","last_tested_version":"1.0.14","endpoint_name":null,"is_component":false}